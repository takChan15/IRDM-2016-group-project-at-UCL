{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script is used to train models using training data\n",
    "# Read the comments at each cell to use it\n",
    "\n",
    "# Many ideas for this script were based on SciKit tutorial on how to do hyper parameter tuning. Below are the links\n",
    "#   http://scikit-learn.org/stable/modules/grid_search.html\n",
    "#   http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html\n",
    "#   http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html\n",
    "#   http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function to do some data preprocessing\n",
    "def transformDf(a_df):\n",
    "    a_df.loc[:, \"norm_myTfIdf_all\"] = a_df.loc[:, \"myTfIdf_all\"] / a_df.loc[:, \"sizeOfQuery\"]\n",
    "    a_df.loc[:, \"norm_myTfIdf_title\"] = a_df.loc[:, \"myTfIdf_title\"] / a_df.loc[:, \"sizeOfQuery\"]\n",
    "    a_df.loc[:, \"norm_myTfIdf_desc\"] = a_df.loc[:, \"myTfIdf_desc\"] / a_df.loc[:, \"sizeOfQuery\"]\n",
    "    a_df.loc[:, \"norm_myTfIdf_attrib\"] = a_df.loc[:, \"myTfIdf_attrib\"] / a_df.loc[:, \"sizeOfQuery\"]\n",
    "\n",
    "    brandMatches_df = pd.get_dummies(a_df.loc[:, \"brandMatches\"])\n",
    "    brandMatches_df.columns = [\"query_product_brands_noMatch\", \"query_has_no_brand\", \"query_product_brands_match\"]\n",
    "    a_df.loc[:, \"query_product_brands_noMatch\"] = brandMatches_df.loc[:, \"query_product_brands_noMatch\"]\n",
    "    a_df.loc[:, \"query_has_no_brand\"] = brandMatches_df.loc[:, \"query_has_no_brand\"]\n",
    "    a_df.loc[:, \"query_product_brands_match\"] = brandMatches_df.loc[:, \"query_product_brands_match\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set here the location of your feature file\n",
    "df = pd.read_csv(\"/Users/taklumbo/Ucl_assignments/IRDM/HomeDepot/featureFiles/train_set/MergedFeatures.csv\", \n",
    "                 delimiter=\",\")\n",
    "transformDf(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This part separate data for the cross validation training. \n",
    "# You don't have to change anything here\n",
    "features = ['queryId', 'productId', 'productTitle', 'originalQuery', 'ratioNumberOfQueryTermsIn_title', 'ratioNumberOfExpQueryTermsIn_all', 'productRank_title','ratioNumberOfExpQueryTermsIn_attrib','ratioNumberOfExpQueryTermsIn_desc', 'ratioNumberOfExpQueryTermsIn_title', 'productRank_all', 'sizeOfExpQuery_title', 'ratioNumberOfQueryTermsIn_desc', 'productRank_desc', 'ratioNumberOfQueryTermsIn_all', 'ratioNumberOfQueryTermsIn_attrib', 'productRank_attrib', 'sizeOfQuery', 'sizeOfExpQuery_attrib', 'sizeOfExpQuery_desc', 'sizeOfExpQuery_all', \"norm_myTfIdf_all\", \"norm_myTfIdf_title\", \"norm_myTfIdf_desc\", \"norm_myTfIdf_attrib\", \"norm_myTfIdf_expQuery_all\", \"norm_myTfIdf_expQuery_title\", \"norm_myTfIdf_expQuery_desc\", \"norm_myTfIdf_expQuery_attrib\", \"docLength_all\", \"docLength_title\", \"docLength_desc\", \"docLength_attrib\", \"query_product_brands_noMatch\", \"query_has_no_brand\", \"query_product_brands_match\", \"sim_score_all\", \"sim_rank_all\"]\n",
    "\n",
    "xFeatures = features[4:]\n",
    "y = df.loc[:, 'y'].values\n",
    "feat_train, feat_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    df.loc[:, features], y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train a StandardScaler instance which is use to normalize the train and test data.\n",
    "# You don't have to change anything here\n",
    "scaler = preprocessing.StandardScaler().fit(feat_train.loc[:, xFeatures])\n",
    "X_train = scaler.transform(feat_train.loc[:, xFeatures])\n",
    "X_test = scaler.transform(feat_test.loc[:, xFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVR-rbf\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Set the variable modelToRun to train different models\n",
    "#\n",
    "# RF = Random forest\n",
    "# SVR-rbf = Support vector machine with Gaussian kernel\n",
    "# GB = Gradient boosting\n",
    " \n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "\n",
    "modelToRun = \"SVR-rbf\"\n",
    "model = None\n",
    "if modelToRun == \"RF\":\n",
    "    print(\"Training Random Forest\")\n",
    "    params = {'min_samples_leaf': 10, 'bootstrap': False, 'max_features': 11, \n",
    "              'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'verbose': 1}\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "if modelToRun == \"GB\":\n",
    "    print(\"Training Gradient Boosting\")\n",
    "\n",
    "    params = {'max_depth': 6, 'learning_rate': 0.05, 'max_features': 1.0, 'n_estimators': 300, \n",
    "              'min_samples_leaf': 17, 'verbose': 1}\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "if modelToRun == \"SVR-rbf\":\n",
    "    print(\"Training SVR-rbf\")\n",
    "    model = svm.SVR(verbose=True, C = 0.5, gamma = 0.03125)\n",
    "    model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.46791180492810563   MAE: 0.370345964199\n"
     ]
    }
   ],
   "source": [
    "# Run this part to calculate the RMSE error for your trained model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "MSE = mean_squared_error(y_test ,model.predict(X_test))\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(\"RMSE: \" + str(RMSE) + \n",
    "      \"   MAE: \" + str(mean_absolute_error(y_test,model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SVR' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-146802866374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatureImportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfeatureImportance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxFeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureImportance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVR' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# Run this cell to show the importance of each feature. Only available for\n",
    "# random forest and gradient boosting\n",
    "featureImportance = {}\n",
    "for i in range(0, len(xFeatures)):\n",
    "    featureImportance[xFeatures[i]] = model.feature_importances_[i]\n",
    "\n",
    "table = pd.Series(featureImportance)\n",
    "sortedTable = table.sort_values(ascending=False)\n",
    "sortedTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reportPredictions(fittedModel, X, X_features):\n",
    "    result = []\n",
    "    predictions = fittedModel.predict(X)\n",
    "    #print(str(len(X_features)) + \" \" + str(predictions.size))\n",
    "    for i in range(0, len(X_features)):\n",
    "        queryId = str(X_features.iloc[i, 0])\n",
    "        aPred = predictions[i]\n",
    "        if aPred > 3:\n",
    "            aPred = 3\n",
    "        result.append(queryId + \",\" + str(aPred))\n",
    "    return result\n",
    "\n",
    "def reportPredictionsWithDetail(fittedModel, X, X_features, y):\n",
    "    result = []\n",
    "    predictions = fittedModel.predict(X)\n",
    "    #print(str(len(X_features)) + \" \" + str(predictions.size))\n",
    "    for i in range(0, len(X_features)):\n",
    "        aPred = predictions[i]\n",
    "        if aPred > 3:\n",
    "            aPred = 3\n",
    "        row = str(aPred) + \",\" + str(y[i])\n",
    "        for j in range(0, X_features.columns.size):\n",
    "            row = row + \",\" + str(X_features.iloc[i, j])\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set this path to your test feature file\n",
    "test_df = pd.read_csv(\"/Users/taklumbo/Ucl_assignments/IRDM/HomeDepot/featureFiles/test_set/MergedFeatures.csv\", \n",
    "                 delimiter=\",\")\n",
    "transformDf(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratioNumberOfExpQueryTermsIn_desc\n",
      "ratioNumberOfExpQueryTermsIn_title\n",
      "productRank_all\n",
      "sizeOfExpQuery_title\n",
      "ratioNumberOfQueryTermsIn_desc\n",
      "productRank_desc\n",
      "ratioNumberOfQueryTermsIn_all\n",
      "ratioNumberOfQueryTermsIn_attrib\n",
      "productRank_attrib\n",
      "sizeOfQuery\n",
      "sizeOfExpQuery_attrib\n",
      "sizeOfExpQuery_desc\n",
      "sizeOfExpQuery_all\n",
      "norm_myTfIdf_all\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-536b0a32029b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#errors = pd.isnull(test_df.loc[:, xFeatures]) #.to_csv(path_or_buf=\"/Users/taklumbo/Ucl_assignments/IRDM/HomeDepot/Result/Null.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    578\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    579\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    396\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     52\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     53\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 54\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Debugging code\n",
    "# Useful code to check for Nan column during scaling\n",
    "myFeatures = test_df.loc[:, xFeatures]\n",
    "\n",
    "myFeatures.loc[:, [\"norm_myTfIdf_all\"]].to_csv(path_or_buf=\"/Users/taklumbo/Ucl_assignments/IRDM/HomeDepot/Result/Null.csv\")\n",
    "for i in range(4, len(features)):\n",
    "    print(myFeatures.columns[i])\n",
    "    preprocessing.StandardScaler().fit(myFeatures.iloc[:, i:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this to standarize your test feature and make prediction\n",
    "X_test_submit = scaler.transform(test_df.loc[:, xFeatures])\n",
    "report = reportPredictions(model, X_test_submit, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set here the location in which you want to save your predictions\n",
    "file = open('/Users/taklumbo/Ucl_assignments/IRDM/HomeDepot/Result/test_result_SVM.csv', 'w')\n",
    "file.write('\"id\",\"relevance\"\\n')\n",
    "for i in range(0, len(report)):\n",
    "    file.write(report[i] + \"\\n\")\n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
